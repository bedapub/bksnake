# Public version of the Snakemake workflow for biokit rnaseq pipeline #
"""
No archive
No JBrowse
No metadata MongoDB database
No biokitr report
"""
import sys
import pandas as pd
import subprocess # to get git hash
from os.path import dirname, join

import scripts.funcs as funcs
import scripts.options as options 


# ------------------------------------------------------------------------------
# Specify multiple config files which contains many pipeline parameters
# This will be merged if the option --configfile=<filepath> is specified
configfile: 'config/config.yaml'


# ------------------------------------------------------------------------------
# Declare variables
OD = config['results']
OD_TMP = os.path.join(OD, 'tmp')
OD_BAM = os.path.join(OD, 'bam')
OD_UBAM = os.path.join(OD, 'bam_unsorted')
OD_CRAM = os.path.join(OD, 'cram')
OD_GCT = os.path.join(OD, 'gct')
OD_ANNO = os.path.join(OD, 'annot')
OD_LOG = os.path.join(OD, 'log')
OD_STATS = os.path.join(OD, 'stats')
OD_FASTQ = os.path.join(OD, 'fastq')
OD_FASTQC = os.path.join(OD, 'fastqc')
OD_MULTIQC = os.path.join(OD, 'multiqc_data')
OD_QC = os.path.join(OD, 'qc')
OD_FC = os.path.join(OD, 'fc')
OD_METRICS = os.path.join(OD, 'metrics')
OD_BW = os.path.join(OD, 'bw')
OD_CUTADAPT = os.path.join(OD, 'cutadapt')
OD_VCF = os.path.join(OD, 'vcf')

SAMPLES = os.path.join(OD, 'samples.txt')
PHENODATA = os.path.join(OD_ANNO, 'phenoData.meta')
METADATA = os.path.join(OD, 'metadata.txt')
MAPSTATS = os.path.join(OD_STATS, 'mapping_stats.txt')


# ------------------------------------------------------------------------------
# Gene length file, columns are:
#
# gene    mean    median  longest_isoform merged
# 1       2       3       4               5
#
# used for tpm calculation, 4th column of geneLength file is used for tpm calculation.
#
GENE_LENGTH_COLUMN_INDEX = 4 


# ------------------------------------------------------------------------------
# Make dataframe with metadata from tab-delimited flat file
if os.path.exists(config['metadata']['file']):
    meta = funcs.get_metadata_from_file(
        config['metadata']['file'],
        config['metadata']['group_name'])
else:
    raise Exception('Metadata file is missing. Abort! '+config['metadata']['file'])
 
 
# Make dataframe with locators for all fastq files
# --> give to fastq files new name, from #ID column !
# Here, use path/url/location from column 'Source Folder' if existing, otherwise from column 'Raw'
if 'Source Folder' in meta:
    if 'Raw' in meta:
        meta.drop(columns='Raw', inplace=True)
    meta['Raw'] = meta['Source Folder']
else:
    if len(set(meta['Raw'])) != 1:
        sys.stderr.write('\n'.join(set(meta['Raw'])))
        raise SystemExit('Ambigious raw data storage URIs/paths, use "Source Folder" to specify multiple locations')


"""
Determine/update library type: paired-end or single-end
in the past this was given in the input config file
here, we overwrite the value in the config if present
by the value obtained from the metadata (via Fastq files)
"""
config = funcs.update_library_type(config, meta)


"""
Determine organism and update config dictionary
Updates config parameters:
1. species: 'hg38'
2. species_name: 'human'
3. organism: 'Homo sapiens'
4. genome_dir: '/path/to/genome/directory/hg38'
5. biokitr gene
"""
config = funcs.update_organism(config, meta)


"""
Determine strand orientation of the sequencing library

Normally this value should come from the metadata as True or False

However, the strand may have 3 options:
 1. SECOND_READ_TRANSCRIPTION_STRAND (default, most common) 
 2. FIRST_READ_TRANSCRIPTION_STRAND 
 3. NONE
 
Strategy:
 1. if present and valid in config then use it from config
 2. if present and valid from metadata then it from file
 3. raise error
"""
PICARD_STRAND, BIOKIT_STRAND, FC_STRAND = funcs.strandedness(meta, config)



"""
Determine fastq file locators from metadata
This depends whether paired- or single-end reads

Parses meta dataframe into list of dictionaries for every sample.
Then, add fastq dir to FASTQ1 and/or FASTQ2 columns
"""
locators = funcs.fastq_locators(meta, config)
sample_ids = meta['#ID']
sample_groups = meta['GROUP']
fastq_names = list(locators['name'])
fastq_names_noext = locators['name'].str.replace('.fastq.gz', '', regex=True)


## ------------------------------------------------------------------------------
## for debugging
#print('------------------------------------', file=sys.stderr)
#print('locators=', file=sys.stderr)
#print(locators, file=sys.stderr)
#for col in locators.columns:
#    print('------------------------------------', file=sys.stderr)
#    print('locators[\''+col+'\']=', file=sys.stderr)
#    print(locators[col], file=sys.stderr)


# ------------------------------------------------------------------------------
# genome reference data and annotations
# Specify genome annotation databases if given as list in config input file, e.g. annotations = ['refseq'], 
# Default is to use all available annotations
DBS = config['genomes'][config['species']]['db']
if 'annotations' in config:  
    annotations = set(config['annotations'])
    all_annotations = set(DBS)      
    if not annotations.issubset(all_annotations):
        missing_annotations = annotations - all_annotations
        raise ValueError(f'\n-----\nThe following annotations are missing: {missing_annotations}\n-----\n')
    else:
        DBS = list(annotations)    
    
GENOME_DIR = os.path.normpath(os.path.join(config['genome_dir'], config['genomes'][config['species']]['genome_subdir']))
GENOME_FASTA = os.path.join(GENOME_DIR, config['ref_fasta'])
GENOME_FASTA_GZ = os.path.join(GENOME_DIR, config['ref_fasta_gz'])
GENOME_FAI = os.path.join(GENOME_DIR, config['ref_fasta_fai'])
GENOME_GZI = os.path.join(GENOME_DIR, config['ref_fasta_gzi'])
GENOME_SIZE = os.path.join(GENOME_DIR, config['genome_size'] )
GENOME_DICT = os.path.join(GENOME_DIR, config['genome_dict'] )
RIBO_INTERVALS = os.path.join(GENOME_DIR, config['ribo_intervals'])
#STAR_DIR = os.path.join(GENOME_DIR, config['star_version'])
STAR_DIR = os.path.join(GENOME_DIR, config['star_version'], DBS[0])
GTF_FOR_STAR_MAPPING = os.path.join(GENOME_DIR, 'gtf', DBS[0], DBS[0]+'.exons.gtf') # must not be gzipped

# check some of these paths
if not os.path.isdir(GENOME_DIR):
    raise Exception('(root) genome directory is not a valid directory. Please check parameter \'genome dir\'.')


# ------------------------------------------------------------------------------
# rules
include: 'rules/bam.smk'
include: 'rules/delete.smk'
include: 'rules/fastq.smk'
include: 'rules/fastqc.smk'
include: 'rules/fc.smk'
include: 'rules/gct.smk'
include: 'rules/picard.smk'
include: 'rules/qc.smk'
include: 'rules/reference.smk'
include: 'rules/snp.smk'
include: 'rules/star.smk'


# ------------------------------------------------------------------------------
# Declare local rules, ie not submitted to the cluster
localrules: all, \
            delete_fastq, \
            delete_bam, \
            mapping_stats, \
            metadata, \
            no_cutadapt, \
            processing_done, \
            star_stats, \
            touch_bw            
            
            
# ------------------------------------------------------------------------------
rule all:
    input:
        os.path.join(OD_ANNO, 'processing.done')


# ----------------------------------------------------------------------------
rule metadata:
    output:
        {SAMPLES},
        {PHENODATA},
        temp(expand(os.path.join(OD_TMP, '{name}.input'), name=locators.name)),
    log:
        os.path.join(OD_LOG, 'metadata.log')
    threads: 1
    resources:
        mem_mb=1000
    run:
        meta.to_csv(output[0], sep='\t', index=False)                  # Make {SAMPLES} file
        funcs.translate_biokit_to_phenoData_meta(output[0], output[1]) # Make {PHENODATA} file
        for name in locators.name:
            Path(os.path.join(OD_TMP, name+'.input')).touch()


# ----------------------------------------------------------------------------
"""
bksnake or vcsnake. Currently, only "bksnake" is supported.
here, we list all files necessary for the pipeline to have generated
prior to archive the data.
this also deletes all "empty" files, ie of size 0, in the "log" folder
"""
if config['pipeline'] == 'bksnake':
    rule processing_done:
        input:
            # Count data
            expand(os.path.join(OD_GCT, '{db}_tpm_collapsed.gct'), db=DBS),
            #
            # QC reports
            expand(os.path.join(OD, '{db}_multiqc_report.html'), db=DBS),
            expand(os.path.join(OD_QC, '{db}_log2tpm_pca.png'), db=DBS),
            expand(os.path.join(OD_QC, '{db}_bioQC.png'), db=DBS),
            #
            # Other optional output files (unmapped, bw, etc)
            expand(os.path.join(OD_CUTADAPT, '{sample}.report.txt'), sample=sample_ids),
            options.get_optional_output_files(sample_ids, config)
        output:
            os.path.join(OD_ANNO, 'processing.done')
        shell:
            """
            find {OD_LOG} -type f -empty -print -delete && touch {output[0]}
            """
elif config ['pipeline'] == 'vcsnake':
    rule processing_done:
        input:
            # QC reports
            os.path.join(OD, 'multiqc_report.html'),
            os.path.join(OD_VCF, 'allSamples_merged.vcf.gz'),
            os.path.join(OD_VCF, 'allSamples_merged.vcf.gz.tbi'),

            # Other optional output files (fastq)
            options.get_optional_output_files_vc(sample_ids, config)
        output:
            os.path.join(OD_ANNO, 'processing.done')
        shell:
            """
            find {OD_LOG} -type f -empty -print -delete && touch {output[0]}
            """
else:
    sys.exit('Variable \'pipeline\' not defined in input config yaml file.')



# ----------------------------------------------------------------------------
"""
Snakemake Report
by using the snakemake option --report
"""
report: 'report/description.rst'

# Add some useful Snakemake variables for the "report" to the config dict
# so that they can be used in the file "workflow/report/description.rst"
config['TOTAL_NUMBER_OF_SAMPLES'] = len(sample_ids)
config['TOTAL_NUMBER_OF_GROUPS'] = len(sample_groups)
